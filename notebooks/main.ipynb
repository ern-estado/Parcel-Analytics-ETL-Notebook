{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "07a80fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/06 01:51:13 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    from_json, col, to_date, concat_ws, lit, monotonically_increasing_id,\n",
    "    year, month, dayofmonth, dayofweek, quarter\n",
    ")\n",
    "from pyspark.sql.types import IntegerType, BooleanType, TimestampType, StringType\n",
    "\n",
    "# === Start Spark session ===\n",
    "spark = SparkSession.builder.appName(\"InPostDataModel_Full\").getOrCreate()\n",
    "\n",
    "# === Step 1: Read input CSV ===\n",
    "df = spark.read.option(\"header\", True) \\\n",
    "               .option(\"escape\", '\"') \\\n",
    "               .option(\"multiLine\", True) \\\n",
    "               .csv(\"/workspaces/inpost_analytics/input/Dane_zadanie_rekrutacyjne.csv\")\n",
    "\n",
    "# === Step 2: Dedupe and parse JSON ===\n",
    "df_unique = df.dropDuplicates()\n",
    "\n",
    "json_sample = df_unique.select(\"event\").filter(col(\"event\").isNotNull()).limit(1).collect()[0][0]\n",
    "inferred_schema = spark.read.json(spark.sparkContext.parallelize([json_sample])).schema\n",
    "\n",
    "df_parsed = df_unique.withColumn(\"event_parsed\", from_json(col(\"event\"), inferred_schema))\n",
    "df_parsed_valid = df_parsed.filter(col(\"event_parsed\").isNotNull())\n",
    "\n",
    "# === Step 3: Flatten fact table ===\n",
    "fact = df_parsed_valid.select(\n",
    "    col(\"event_parsed.entry_date\").cast(TimestampType()).alias(\"entry_date\"),\n",
    "    col(\"event_parsed.event_code\").cast(StringType()).alias(\"event_code\"),\n",
    "    col(\"event_parsed.event_date\").cast(TimestampType()).alias(\"event_date\"),\n",
    "    col(\"event_parsed.event_nature\").cast(StringType()).alias(\"event_nature\"),\n",
    "    col(\"event_parsed.event_sub_code\").cast(StringType()).alias(\"event_sub_code\"),\n",
    "\n",
    "    col(\"event_parsed.shipping.sign_code\").cast(StringType()).alias(\"shipping_sign_code\"),\n",
    "    col(\"event_parsed.shipping.brand_code_alpha\").cast(StringType()).alias(\"shipping_brand_code_alpha\"),\n",
    "\n",
    "    col(\"event_parsed.shipping.collection.prestation_code\").cast(StringType()).alias(\"shipping_collection_prestation_code\"),\n",
    "    col(\"event_parsed.shipping.collection.round.codeAgence\").cast(IntegerType()).alias(\"shipping_collection_round_codeAgence\"),\n",
    "    col(\"event_parsed.shipping.collection.round.pays\").cast(StringType()).alias(\"shipping_collection_round_pays\"),\n",
    "\n",
    "    col(\"event_parsed.shipping.sav_folder\").cast(BooleanType()).alias(\"shipping_sav_folder\"),\n",
    "    col(\"event_parsed.shipping.parcel_number\").cast(IntegerType()).alias(\"shipping_parcel_number\"),\n",
    "    col(\"event_parsed.shipping.parcel_sequence\").cast(IntegerType()).alias(\"shipping_parcel_sequence\"),\n",
    "    col(\"event_parsed.shipping.is_replaced\").cast(BooleanType()).alias(\"shipping_is_replaced\"),\n",
    "    col(\"event_parsed.shipping.shipping_id\").cast(StringType()).alias(\"shipping_shipping_id\"),\n",
    "    col(\"event_parsed.shipping.shipping_number\").cast(StringType()).alias(\"shipping_shipping_number\"),\n",
    "\n",
    "    col(\"event_parsed.shipping.delivery.prestation_code\").cast(StringType()).alias(\"shipping_delivery_prestation_code\"),\n",
    "    col(\"event_parsed.shipping.delivery.round.codeAgence\").cast(IntegerType()).alias(\"shipping_delivery_round_codeAgence\"),\n",
    "    col(\"event_parsed.shipping.delivery.round.pays\").cast(StringType()).alias(\"shipping_delivery_round_pays\"),\n",
    "\n",
    "    col(\"event_parsed.shipping.state.code\").cast(StringType()).alias(\"shipping_state_code\"),\n",
    "    col(\"event_parsed.shipping.state.date\").cast(TimestampType()).alias(\"shipping_state_date\"),\n",
    "    col(\"event_parsed.shipping.state.nature\").cast(StringType()).alias(\"shipping_state_nature\"),\n",
    "    col(\"event_parsed.shipping.state.sousCode\").cast(StringType()).alias(\"shipping_state_sousCode\"),\n",
    "\n",
    "    to_date(col(\"event_parsed.event_date\")).alias(\"event_date_only\")\n",
    ")\n",
    "\n",
    "# === Step 4: Write fact table to disk ===\n",
    "fact.coalesce(1).write.mode(\"overwrite\").csv(\"/workspaces/inpost_analytics/output/fact_shipping_events.csv\", header=True)\n",
    "\n",
    "# === Step 5: Build dim_customer ===\n",
    "dim_customer = fact.select(\n",
    "    col(\"shipping_brand_code_alpha\"),\n",
    "    col(\"shipping_sign_code\")\n",
    ").distinct() \\\n",
    ".withColumn(\"customer_key\", concat_ws(\"_\", col(\"shipping_brand_code_alpha\"), col(\"shipping_sign_code\")))\n",
    "\n",
    "dim_customer.select(\n",
    "    col(\"customer_key\"),\n",
    "    col(\"shipping_brand_code_alpha\").alias(\"brand_code_alpha\"),\n",
    "    col(\"shipping_sign_code\").alias(\"sign_code\")\n",
    ").coalesce(1).write.mode(\"overwrite\").csv(\"/workspaces/inpost_analytics/output/dim_customer.csv\", header=True)\n",
    "\n",
    "# === Step 6: Build dim_event_type ===\n",
    "dim_event_type = fact.select(\n",
    "    col(\"event_code\"),\n",
    "    col(\"event_sub_code\"),\n",
    "    col(\"event_nature\").alias(\"event_description\")\n",
    ").distinct()\n",
    "\n",
    "dim_event_type.coalesce(1).write.mode(\"overwrite\").csv(\"/workspaces/inpost_analytics/output/dim_event_type.csv\", header=True)\n",
    "\n",
    "# === Step 7: Build dim_location ===\n",
    "collection_locations = fact.select(\n",
    "    col(\"shipping_collection_round_codeAgence\").alias(\"codeAgence\"),\n",
    "    col(\"shipping_collection_round_pays\").alias(\"pays\")\n",
    ").distinct() \\\n",
    ".withColumn(\"location_type\", lit(\"collection\"))\n",
    "\n",
    "delivery_locations = fact.select(\n",
    "    col(\"shipping_delivery_round_codeAgence\").alias(\"codeAgence\"),\n",
    "    col(\"shipping_delivery_round_pays\").alias(\"pays\")\n",
    ").distinct() \\\n",
    ".withColumn(\"location_type\", lit(\"delivery\"))\n",
    "\n",
    "dim_location = collection_locations.union(delivery_locations) \\\n",
    "    .dropDuplicates([\"codeAgence\", \"pays\", \"location_type\"]) \\\n",
    "    .withColumn(\"location_id\", monotonically_increasing_id())\n",
    "\n",
    "dim_location.select(\n",
    "    col(\"location_id\"),\n",
    "    col(\"codeAgence\"),\n",
    "    col(\"pays\"),\n",
    "    col(\"location_type\")\n",
    ").coalesce(1).write.mode(\"overwrite\").csv(\"/workspaces/inpost_analytics/output/dim_location.csv\", header=True)\n",
    "\n",
    "# === Step 8: Build dim_date ===\n",
    "dim_date = fact.select(\n",
    "    col(\"event_date_only\").alias(\"date_key\")\n",
    ").distinct() \\\n",
    ".withColumn(\"year\", year(col(\"date_key\"))) \\\n",
    ".withColumn(\"quarter\", quarter(col(\"date_key\"))) \\\n",
    ".withColumn(\"month\", month(col(\"date_key\"))) \\\n",
    ".withColumn(\"day\", dayofmonth(col(\"date_key\"))) \\\n",
    ".withColumn(\"day_of_week\", dayofweek(col(\"date_key\")))\n",
    "\n",
    "dim_date.coalesce(1).write.mode(\"overwrite\").csv(\"/workspaces/inpost_analytics/output/dim_date.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "cc5e3779",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/06 01:51:36 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średni czas dostawy paczki (dni): 3.58\n",
      "Średni całkowity czas życia paczki (dni): 4.67\n",
      "Średni czas odbioru paczki z docelowego punktu (dni): 1.09\n",
      "Całkowita liczba paczek: 46019\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, datediff, min as spark_min, max as spark_max, avg, countDistinct\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"InPostMetrics\").getOrCreate()\n",
    "\n",
    "# Load fact table (adjust path if needed)\n",
    "fact = spark.read.option(\"header\", True) \\\n",
    "    .option(\"inferSchema\", True) \\\n",
    "    .csv(\"/workspaces/inpost_analytics/output/fact_shipping_events.csv\")\n",
    "\n",
    "# Filter and get event dates by event type per package\n",
    "sender_events = fact.filter(\n",
    "    (col(\"event_code\") == \"PEC\") & \n",
    "    (col(\"event_sub_code\").isin(\"APM\", \"REL\"))\n",
    ").select(\n",
    "    col(\"shipping_shipping_id\"),\n",
    "    col(\"event_date\").alias(\"sender_placement_date\")\n",
    ")\n",
    "\n",
    "courier_events = fact.filter(\n",
    "    (col(\"event_code\") == \"TRN\") & \n",
    "    (col(\"event_sub_code\").isin(\"APM\", \"REL\"))\n",
    ").select(\n",
    "    col(\"shipping_shipping_id\"),\n",
    "    col(\"event_date\").alias(\"courier_placement_date\")\n",
    ")\n",
    "\n",
    "recipient_events = fact.filter(\n",
    "    col(\"event_code\") == \"LIV\"\n",
    ").select(\n",
    "    col(\"shipping_shipping_id\"),\n",
    "    col(\"event_date\").alias(\"recipient_pickup_date\")\n",
    ")\n",
    "\n",
    "# Aggregate dates per package: get min event date if multiple\n",
    "from pyspark.sql.functions import first\n",
    "\n",
    "sender_agg = sender_events.groupBy(\"shipping_shipping_id\") \\\n",
    "    .agg(spark_min(\"sender_placement_date\").alias(\"sender_placement_date\"))\n",
    "\n",
    "courier_agg = courier_events.groupBy(\"shipping_shipping_id\") \\\n",
    "    .agg(spark_min(\"courier_placement_date\").alias(\"courier_placement_date\"))\n",
    "\n",
    "recipient_agg = recipient_events.groupBy(\"shipping_shipping_id\") \\\n",
    "    .agg(spark_min(\"recipient_pickup_date\").alias(\"recipient_pickup_date\"))\n",
    "\n",
    "# Join all dates together on package ID\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "dfs = [sender_agg, courier_agg, recipient_agg]\n",
    "\n",
    "fact_dates = reduce(\n",
    "    lambda left, right: left.join(right, on=\"shipping_shipping_id\", how=\"inner\"), dfs\n",
    ")\n",
    "\n",
    "# Calculate differences in days\n",
    "fact_dates = fact_dates.withColumn(\n",
    "    \"delivery_time_days\", datediff(col(\"courier_placement_date\"), col(\"sender_placement_date\"))\n",
    ").withColumn(\n",
    "    \"total_lifetime_days\", datediff(col(\"recipient_pickup_date\"), col(\"sender_placement_date\"))\n",
    ").withColumn(\n",
    "    \"pickup_time_days\", datediff(col(\"recipient_pickup_date\"), col(\"courier_placement_date\"))\n",
    ")\n",
    "\n",
    "# Calculate averages\n",
    "avg_delivery_time = fact_dates.select(avg(\"delivery_time_days\")).collect()[0][0]\n",
    "avg_total_lifetime = fact_dates.select(avg(\"total_lifetime_days\")).collect()[0][0]\n",
    "avg_pickup_time = fact_dates.select(avg(\"pickup_time_days\")).collect()[0][0]\n",
    "\n",
    "# Total unique packages\n",
    "total_packages = fact.select(countDistinct(\"shipping_shipping_id\")).collect()[0][0]\n",
    "\n",
    "print(f\"Średni czas dostawy paczki (dni): {avg_delivery_time:.2f}\")\n",
    "print(f\"Średni całkowity czas życia paczki (dni): {avg_total_lifetime:.2f}\")\n",
    "print(f\"Średni czas odbioru paczki z docelowego punktu (dni): {avg_pickup_time:.2f}\")\n",
    "print(f\"Całkowita liczba paczek: {total_packages}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e684d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
